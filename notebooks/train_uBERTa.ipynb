{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce7feefc-15c6-4236-bc25-33ffa487c815",
   "metadata": {},
   "source": [
    "# Train uBERTa\n",
    "\n",
    "Here, we'll use the pretrained model for fine-tuning on the token-level classification objective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0540ffd6-8182-4fc3-848a-ac8a8f2e32c1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Prerequisites\n",
    "\n",
    "This notebook requires:\n",
    "- [DS_BASE.tsv](https://drive.google.com/file/d/1gPjOoxWOAPpfPmKFbQlVT0hncjIpst5E/view?usp=sharing) (`../data/DS_BASE.tsv`)\n",
    "- [dataset_labeling.tsv](https://drive.google.com/file/d/1z_dQtERIPvf_ZqnGLCNGLx_l6GcKHJZ1/view?usp=sharing) (`../data/dataset_labeling.tsv`)\n",
    "- [pretrained_model](https://drive.google.com/file/d/1fiYXNEOUsqSHGbzwbuWKAu5e26-q0UM1/view?usp=sharing) (`../models/ws100_step50_pretrain`)\n",
    "\n",
    "One can either download or obtain the requirements manually: `prepare_base_dataset.ipynb` for the first two, and `pretrain_distil.ipynb` for the pretrained model. \n",
    "\n",
    "For instance, to download the data, starting from the project's root:\n",
    "```bash\n",
    "gdown --fuzzy https://drive.google.com/file/d/1gPjOoxWOAPpfPmKFbQlVT0hncjIpst5E/view?usp=sharing\n",
    "gdown --fuzzy https://drive.google.com/file/d/1z_dQtERIPvf_ZqnGLCNGLx_l6GcKHJZ1/view?usp=sharing\n",
    "tar -xzf DS_BASE.tsv.tar.gz\n",
    "tar -xzf dataset_labeling.tsv.tar.gz\n",
    "mkdir -p ../models\n",
    "cd ../models\n",
    "gdown --fuzzy https://drive.google.com/file/d/1fiYXNEOUsqSHGbzwbuWKAu5e26-q0UM1/view?usp=sharing\n",
    "tar -xzf ws100_step50_pretrain.tar.gz\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dce13b-9f9a-4b72-aa1e-8a15c0afc01d",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c762b75-6ad5-45ed-a788-81c9ff207960",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from transformers import DistilBertConfig, get_linear_schedule_with_warmup\n",
    "\n",
    "from uBERTa.loader import uBERTaLoader\n",
    "from uBERTa.model import uBERTa_classifier, WeightedDistilBertClassifier\n",
    "from uBERTa.tokenizer import DNATokenizer\n",
    "from uBERTa.utils import split_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e32dd6af-0b41-4d53-9865-1ebe5a0bf730",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_existing(paths):\n",
    "    loader = (\n",
    "        lambda p: None if not p.exists() else \n",
    "        (pd.read_hdf(p) if p.suffix == '.h5' else torch.load(p))\n",
    "    )\n",
    "    return {k: loader(v) for k, v in paths.items()}\n",
    "\n",
    "def parse_base(path_base, path_labels, min_seq_size):\n",
    "    df = pd.read_csv(path_base, sep='\\t')\n",
    "    df['SeqSize'] = df['Seq'].apply(len)\n",
    "    print(f'Initial ds: {len(df)}')\n",
    "    df = df.merge(pd.read_csv(path_labels, sep='\\t'), on='GeneID')\n",
    "    print(f'Labeled genes: {len(df)}')\n",
    "    df = df[df.SeqSize >= min_seq_size]\n",
    "    print(f'Conforming to size threshold: {len(df)}')\n",
    "    split_values(df, 'SeqEnum')\n",
    "    split_values(df, 'SeqEnumPositive')\n",
    "    split_values(df, 'Classes')\n",
    "    split_values(df, 'Signal', dtype=float)\n",
    "    return df\n",
    "\n",
    "def calc_scheduler_steps(loader, warmup_perc=0.1, max_epochs=100):\n",
    "    epoch_steps = len(loader.train_dataloader())\n",
    "    total_steps = epoch_steps * max_epochs\n",
    "    warmup_steps = int(warmup_perc * total_steps)\n",
    "    return warmup_steps, total_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36982b1b-edd3-480f-be22-98c35157a249",
   "metadata": {},
   "outputs": [],
   "source": [
    "KMER = 3\n",
    "MIN_SEQ_SIZE = 10\n",
    "WINDOW = 100\n",
    "STEP = WINDOW // 4\n",
    "MAX_EPOCHS = 30\n",
    "\n",
    "# Valid start codons\n",
    "STARTS = ('ACG', 'ATC', 'ATG', 'ATT', 'CTG', 'GTG')\n",
    "# Name of the dataset\n",
    "DS = f'ws{WINDOW}_step{STEP}_{\"_\".join(STARTS)}'\n",
    "# Name of the model\n",
    "MODEL = f'{DS}_pretrain_tokenlevel_signal'\n",
    "\n",
    "MODEL_PATH = Path(f'../models/{MODEL}')\n",
    "PRETRAINED_PATH = Path('../models/ws100_step50_pretrain/')\n",
    "PATH_BASE_DS = Path('../data/DS_BASE.tsv')\n",
    "PATH_LABELS = Path('../data/dataset_labeling.tsv')\n",
    "\n",
    "datasets_base = Path(f'../data/datasets/{DS}')\n",
    "datasets_base.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "DATASETS = {\n",
    "    'train_ds': datasets_base / 'train_ds.h5',\n",
    "    'val_ds': datasets_base / 'val_ds.h5',\n",
    "    'test_ds': datasets_base / 'test_ds.h5',\n",
    "    'train_tds': datasets_base / 'train_tds.bin',\n",
    "    'val_tds': datasets_base / 'val_tds.bin',\n",
    "    'test_tds': datasets_base / 'test_tds.bin'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e135b9d3-ea93-4eeb-9bb8-99ade2eef209",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3318ae-ba4f-4ded-9b66-dc44067d3635",
   "metadata": {},
   "source": [
    "## Prepare datasets\n",
    "Preparing datasets is time consuming. The code below will check if the required datasets are present in `datasets_base`: if not (e.g., first time running the notebook), the process will start from the base dataset. The data preparation is fully encapsulated into the `setup` method of `uBERTaLoader`. Basically, this will kmerize sequence data, compose token labels, aggregate the ribo-seq signal for each token, and use the sliding window on sequences to unify the input size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e5a09d7-7297-438a-8639-47bc0ce7c1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial ds: 79677\n",
      "Labeled genes: 19652\n",
      "Conforming to size threshold: 19377\n"
     ]
    }
   ],
   "source": [
    "ds_paths = (DATASETS['train_ds'], DATASETS['val_ds'], DATASETS['test_ds'])\n",
    "if any(not p.exists() for p in ds_paths):\n",
    "    ds = parse_base(PATH_BASE_DS, PATH_LABELS, MIN_SEQ_SIZE)\n",
    "else:\n",
    "    ds = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be0d81c9-8cba-4fdf-b2e1-8f798baea1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DNATokenizer(kmer=KMER)\n",
    "loader = uBERTaLoader(\n",
    "    ds, WINDOW, STEP, tokenizer, \n",
    "    **load_existing(DATASETS),\n",
    "    scale_signal_bounds=(0.0, 10.0),\n",
    "    is_mlm_task=False,\n",
    "    valid_start_codons=STARTS,\n",
    "    batch_size=2 ** 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aead4527-b657-49bc-8a91-ef2da82242ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:uBERTa.loader:Total initial sequences: 19377\n",
      "INFO:uBERTa.loader:Split datasets. Train: 15459, Val: 1924, Test: 1994.\n",
      "INFO:uBERTa.loader:Preparing Train with 15459 records for token-level task\n",
      "INFO:uBERTa.loader:Using kmer 3 on ('Seq', 'SeqEnum', 'Signal', 'Classes')\n",
      "DEBUG:uBERTa.loader:Reducing kmers for Train\n",
      "/home/ivan/miniconda3/envs/uberta/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3162: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return asarray(a).ndim\n",
      "DEBUG:uBERTa.loader:Filtering to ('ACG', 'ATC', 'ATG', 'ATT', 'CTG', 'GTG') for Train\n",
      "DEBUG:uBERTa.loader:Capping and scaling signal for Train\n",
      "DEBUG:uBERTa.loader:Capped signal in (0.1, 5000.0)\n",
      "DEBUG:uBERTa.loader:Scaled signal between 0 and 1. Min 0.1, Max 5000.0\n",
      "INFO:uBERTa.loader:Rolling window with size 98, step 25\n",
      "WARNING:uBERTa.loader:Removing 615 out of 117097 windows without classes from Train. Consider calibrating window parameters.\n",
      "INFO:uBERTa.loader:Preparing Val with 1924 records for token-level task\n",
      "INFO:uBERTa.loader:Using kmer 3 on ('Seq', 'SeqEnum', 'Signal', 'Classes')\n",
      "DEBUG:uBERTa.loader:Reducing kmers for Val\n",
      "/home/ivan/miniconda3/envs/uberta/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3162: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return asarray(a).ndim\n",
      "DEBUG:uBERTa.loader:Filtering to ('ACG', 'ATC', 'ATG', 'ATT', 'CTG', 'GTG') for Val\n",
      "DEBUG:uBERTa.loader:Capping and scaling signal for Val\n",
      "DEBUG:uBERTa.loader:Capped signal in (0.1, 5000.0)\n",
      "DEBUG:uBERTa.loader:Scaled signal between 0 and 1. Min 0.1, Max 5000.0\n",
      "INFO:uBERTa.loader:Rolling window with size 98, step 25\n",
      "WARNING:uBERTa.loader:Removing 88 out of 15453 windows without classes from Val. Consider calibrating window parameters.\n",
      "INFO:uBERTa.loader:Preparing Test with 1994 records for token-level task\n",
      "INFO:uBERTa.loader:Using kmer 3 on ('Seq', 'SeqEnum', 'Signal', 'Classes')\n",
      "DEBUG:uBERTa.loader:Reducing kmers for Test\n",
      "/home/ivan/miniconda3/envs/uberta/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3162: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return asarray(a).ndim\n",
      "DEBUG:uBERTa.loader:Filtering to ('ACG', 'ATC', 'ATG', 'ATT', 'CTG', 'GTG') for Test\n",
      "DEBUG:uBERTa.loader:Capping and scaling signal for Test\n",
      "DEBUG:uBERTa.loader:Capped signal in (0.1, 5000.0)\n",
      "DEBUG:uBERTa.loader:Scaled signal between 0 and 1. Min 0.1, Max 5000.0\n",
      "INFO:uBERTa.loader:Rolling window with size 98, step 25\n",
      "WARNING:uBERTa.loader:Removing 91 out of 15890 windows without classes from Test. Consider calibrating window parameters.\n",
      "INFO:uBERTa.loader:Finalized datasets. Train: 116482, Val: 15365, Test: 15799.\n",
      "DEBUG:uBERTa.loader:Prepared tensor datasets\n",
      "/home/ivan/miniconda3/envs/uberta/lib/python3.8/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: 'train_ds.h5'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/ivan/code/uBERTa/uBERTa/loader.py:187: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['TranscriptID', 'ExonID', 'GeneID', 'Chrom', 'Strand',\n",
      "       'SeqEnumPositive', 'Dataset', 'Seq', 'Classes', 'SeqEnum', 'Signal'],\n",
      "      dtype='object')]\n",
      "\n",
      "  ds.to_hdf(str(p), name)\n",
      "/home/ivan/miniconda3/envs/uberta/lib/python3.8/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: 'val_ds.h5'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/ivan/miniconda3/envs/uberta/lib/python3.8/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: 'test_ds.h5'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n"
     ]
    }
   ],
   "source": [
    "loader.setup()\n",
    "loader.save_all(datasets_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490d1c13-6314-45d7-b047-05419adef7a3",
   "metadata": {},
   "source": [
    "Below we'll count unmasked tokens (defined by `STARTS` above) and count positive and negative classes. The `weight` can then be used with `uBERTa_classifier`, although it doesn't lead to performance gains and will likely require manually adjusting the threshold for converting the predictions into binary labels for the desired balance between recall and precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23c2d379-bf94-4ae8-a9c3-a9b890f3ed15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({44: 39445, 60: 25270, 10: 17082, 12: 14558, 11: 14270, 16: 9337}) [ 0.52456622 10.67657529] tensor(114344) tensor(5618)\n",
      "Counter({44: 41275, 60: 25270, 10: 17345, 12: 15340, 11: 13694, 16: 9643}) [ 0.52253562 11.593549  ] tensor(117281) tensor(5286)\n",
      "Counter({44: 300604, 60: 196150, 10: 123232, 12: 105487, 11: 105119, 16: 72656}) [ 0.52224572 11.73811566] tensor(864773) tensor(38475)\n"
     ]
    }
   ],
   "source": [
    "for tds in [loader.val_tds, loader.test_tds, loader.train_tds]:\n",
    "    classes, inp_ids = tds.tensors[2], tds.tensors[0]\n",
    "    mask = classes != -100\n",
    "    mask0 = classes == 0\n",
    "    mask1 = classes == 1\n",
    "    weight = compute_class_weight('balanced', classes=[0, 1], y=classes[mask].numpy())\n",
    "    codons = inp_ids[mask]\n",
    "    print(Counter(codons.numpy()), weight, mask0.sum(), mask1.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfaf217-57d9-4a50-be40-a0b01778bf85",
   "metadata": {},
   "source": [
    "## Setup model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2d051b0-150b-446c-b618-4401f18ef015",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5463, 54630)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warmup_steps, total_steps = calc_scheduler_steps(\n",
    "    loader, warmup_perc=0.1, max_epochs=MAX_EPOCHS)\n",
    "warmup_steps, total_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad446ed-af4f-43b5-9468-15341f0e9648",
   "metadata": {},
   "source": [
    "We'll use the config of the pretrained model and add one field so that the model uses the experimental signal internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cef1578-33be-43ba-b583-e74223d66bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = DistilBertConfig.from_pretrained(PRETRAINED_PATH)\n",
    "config.use_signal = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e8b9e0-537d-40e6-acb0-90a2e6122d57",
   "metadata": {},
   "source": [
    "Note: manually providing the `device` argument is needed only if using `weight`. It will also restrict the model to the provided device, and using distributed training may cause an exception. I couldn't (yet) find a way to use `weight` with distributed training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d974e878-7ba6-4b2a-81de-246b756f5b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../models/ws100_step50_pretrain were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = uBERTa_classifier(\n",
    "    model=WeightedDistilBertClassifier,\n",
    "    config=config,\n",
    "    opt_kwargs={'lr': 1e-5, 'weight_decay': 1.5, 'eps': 1e-8}, \n",
    "    scheduler=get_linear_schedule_with_warmup,\n",
    "    scheduler_kwargs={'num_warmup_steps': warmup_steps, 'num_training_steps': total_steps},\n",
    "    weight=None, device='cuda:1'\n",
    ")\n",
    "# It's a bit nested, most certainly unnecessarily\n",
    "model.model.bert = model.model.bert.from_pretrained(PRETRAINED_PATH)\n",
    "model.model.config.use_signal = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14496ced-afcd-49e1-a46a-9b9136bea7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25540/3704423482.py:1: LightningDeprecationWarning: The `LightningModule.summarize` method is deprecated in v1.5 and will be removed in v1.7. Use `pytorch_lightning.utilities.model_summary.summarize` instead.\n",
      "  model.summarize()\n",
      "/home/ivan/miniconda3/envs/uberta/lib/python3.8/site-packages/pytorch_lightning/utilities/model_summary.py:471: LightningDeprecationWarning: Argument `mode` in `LightningModule.summarize` is deprecated in v1.4 and will be removed in v1.6. Use `max_depth=1` to replicate `mode=top` behavior.\n",
      "  rank_zero_deprecation(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  | Name  | Type                         | Params\n",
       "-------------------------------------------------------\n",
       "0 | model | WeightedDistilBertClassifier | 20.9 M\n",
       "-------------------------------------------------------\n",
       "20.9 M    Trainable params\n",
       "0         Non-trainable params\n",
       "20.9 M    Total params\n",
       "83.657    Total estimated model params size (MB)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bee71b-9063-4627-afda-c9c19bca8f20",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c8fecc7-93b4-4881-a28a-2577501f2c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopper = pl.callbacks.early_stopping.EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    verbose=True, mode='min', \n",
    "    min_delta=1e-6,\n",
    "    patience=10)\n",
    "pointer = pl.callbacks.ModelCheckpoint(\n",
    "    monitor='val_loss', \n",
    "    dirpath=f'../models/checkpoints/{MODEL}', \n",
    "    verbose=True, mode='min')\n",
    "logger = pl.loggers.TensorBoardLogger('../logs', f'{MODEL}')\n",
    "lr_monitor = pl.callbacks.LearningRateMonitor('epoch')\n",
    "bar = pl.callbacks.TQDMProgressBar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4fa478e-ede3-40a0-8431-4ff3c0632856",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "/home/ivan/miniconda3/envs/uberta/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:58: LightningDeprecationWarning: Setting `Trainer(stochastic_weight_avg=True)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.stochastic_weight_avg.StochasticWeightAveraging` directly to the Trainer's `callbacks` argument instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = [1]\n",
    "trainer = pl.Trainer(\n",
    "    gradient_clip_val=0.5, \n",
    "    stochastic_weight_avg=True,\n",
    "    accelerator=\"gpu\",\n",
    "    precision=16,\n",
    "    gpus=gpus,\n",
    "    callbacks=[stopper, pointer, bar, lr_monitor],\n",
    "    logger=logger,\n",
    "    max_epochs=MAX_EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f29384-de38-4c59-ad73-b17d24e98b26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.fit(model, loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d047020b-9fd4-49cc-9f12-a9712f0f7e8a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Reinitialize the model from the best checkpoint and save the weghts.\n",
    "\n",
    "Check the checkpoints directory (`../models/checkpoints/{MODEL}`) and put a name of the desired checkpoint file below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ef7579-9d7b-422d-bb74-b2e34bafaf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = 'epoch=20-step=38597.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4252e2a0-9626-41dc-90c9-7fd8a31f18b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "_model = uBERTa_classifier.load_from_checkpoint(\n",
    "    f'../models/checkpoints/{MODEL}/{ckpt}', \n",
    "    model=WeightedDistilBertClassifier,\n",
    "    config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ecaf54-5e35-4eb1-adf9-71815e4a10e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "_model.model.save_pretrained(MODEL_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
