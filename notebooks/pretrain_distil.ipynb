{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15fea205-382c-458e-839f-1e433e509102",
   "metadata": {},
   "source": [
    "# Pretrain DistilBert\n",
    "\n",
    "In this notebook, we'll pretrain DistilBert model on the MLM objective using all 5'UTR exonic sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba9dd20-46b8-4cf0-bf6d-9b3879e45cc0",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "This notebook requires:\n",
    "- [DS_BASE.tsv](https://drive.google.com/file/d/1gPjOoxWOAPpfPmKFbQlVT0hncjIpst5E/view?usp=sharing)\n",
    "\n",
    "Download and unpack the required data, then provide a path to a base dataset below.\n",
    "\n",
    "For instance, starting in the project's root:\n",
    "\n",
    "```bash\n",
    "cd data\n",
    "gdown --fuzzy https://drive.google.com/file/d/1gPjOoxWOAPpfPmKFbQlVT0hncjIpst5E/view?usp=sharing\n",
    "tar -xzf DS_BASE.tsv.tar.gz\n",
    "rm DS_BASE.tsv.tar.gz\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c762b75-6ad5-45ed-a788-81c9ff207960",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch.utils.data import RandomSampler, SequentialSampler, DataLoader, TensorDataset\n",
    "from transformers import DistilBertConfig, DistilBertForMaskedLM, get_linear_schedule_with_warmup\n",
    "\n",
    "from uBERTa.loader import uBERTaLoader\n",
    "from uBERTa.model import uBERTa_mlm\n",
    "from uBERTa.tokenizer import DNATokenizer\n",
    "from uBERTa.utils import split_values, fill_row_around_ones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa65209-1694-472d-8c47-005e98e843d8",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e32dd6af-0b41-4d53-9865-1ebe5a0bf730",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_existing(paths):\n",
    "    loader = (\n",
    "        lambda p: None if not p.exists() else \n",
    "        (pd.read_hdf(p) if p.suffix == '.h5' else torch.load(p))\n",
    "    )\n",
    "    return {k: loader(v) for k, v in paths.items()}\n",
    "\n",
    "def parse_base(path_base, min_seq_size):\n",
    "    \"\"\"\n",
    "    Read the base dataset, split sequence values and filter by the sequence size.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path_base, sep='\\t')\n",
    "    df['SeqSize'] = df['Seq'].apply(len)\n",
    "    print(f'Initial ds: {len(df)}')\n",
    "    df = df[df.SeqSize >= min_seq_size]\n",
    "    print(f'Conforming to size threshold: {len(df)}')\n",
    "    split_values(df, 'SeqEnum')\n",
    "    split_values(df, 'SeqEnumPositive')\n",
    "    split_values(df, 'Classes')\n",
    "    split_values(df, 'Signal', dtype=float)\n",
    "    return df\n",
    "\n",
    "def calc_scheduler_steps(loader, warmup_perc=0.1, max_epochs=100):\n",
    "    \"\"\"\n",
    "    Calculate warmup steps based on the number of batches in train_dataloader.\n",
    "    \"\"\"\n",
    "    epoch_steps = len(loader.train_dataloader())\n",
    "    total_steps = epoch_steps * max_epochs\n",
    "    warmup_steps = int(warmup_perc * total_steps)\n",
    "    return warmup_steps, total_steps\n",
    "\n",
    "class Loader(pl.LightningDataModule):\n",
    "    \"\"\"\n",
    "    Loader for the MLM objective.\n",
    "    \"\"\"\n",
    "    def __init__(self, tds, batch_size, val_frac=0.05, num_proc=4):\n",
    "        self.batch_size = batch_size\n",
    "        self.val_frac = val_frac\n",
    "        self.tds = tds\n",
    "        self.num_proc = num_proc\n",
    "        \n",
    "        self.val_tds, self.train_tds = None, None\n",
    "    \n",
    "    def setup(self):\n",
    "        idx = np.random.binomial(1, self.val_frac, len(self.tds)) == 1\n",
    "        idx_val = np.where(idx)[0]\n",
    "        idx_train = np.where(~idx)[0]\n",
    "        self.val_tds = TensorDataset(*self.tds[idx_val])\n",
    "        self.train_tds = TensorDataset(*self.tds[idx_train])\n",
    "    \n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.train_tds, sampler=RandomSampler(self.train_tds),\n",
    "            batch_size=self.batch_size, num_workers=self.num_proc)\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.val_tds, sampler=SequentialSampler(self.val_tds),\n",
    "            batch_size=self.batch_size, num_workers=self.num_proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36982b1b-edd3-480f-be22-98c35157a249",
   "metadata": {},
   "outputs": [],
   "source": [
    "KMER = 3\n",
    "WINDOW = 100\n",
    "MIN_SEQ_SIZE = 25\n",
    "STEP = WINDOW // 2\n",
    "MAX_EPOCHS = 100\n",
    "\n",
    "DS = f'ws{WINDOW}_step{STEP}'\n",
    "MODEL = f'{DS}_pretrain'\n",
    "\n",
    "DATA = Path('../data')\n",
    "DATA.mkdir(exist_ok=True)\n",
    "# Path to save the trained model\n",
    "MODEL_PATH = Path(f'../models/{MODEL}')\n",
    "MODEL_PATH.mkdir(exist_ok=True, parents=True)\n",
    "# Path to a base dataset\n",
    "PATH_BASE_DS = DATA / 'DS_BASE.tsv'\n",
    "\n",
    "# Base dir for the prepared datasets\n",
    "datasets_base = DATA / f'datasets/{DS}'\n",
    "datasets_base.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "DATASETS = {\n",
    "    'train_ds': datasets_base / 'train_ds.h5',\n",
    "    'val_ds': datasets_base / 'val_ds.h5',\n",
    "    'test_ds': datasets_base / 'test_ds.h5',\n",
    "    'train_tds': datasets_base / 'train_tds.bin',\n",
    "    'val_tds': datasets_base / 'val_tds.bin',\n",
    "    'test_tds': datasets_base / 'test_tds.bin'\n",
    "}\n",
    "\n",
    "np.random.seed(666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e135b9d3-ea93-4eeb-9bb8-99ade2eef209",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4488ae7-28ba-4e34-a2df-f5a43f1c5148",
   "metadata": {},
   "source": [
    "## Prepare datasets\n",
    "\n",
    "From the base dataset, we prepare the `.h5` dataframes and `TensorDataset` objects, where the latter are used to initialize `Loader`.\n",
    "\n",
    "First, we load and parse the base dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e5a09d7-7297-438a-8639-47bc0ce7c1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial ds: 79677\n",
      "Conforming to size threshold: 75169\n"
     ]
    }
   ],
   "source": [
    "ds_paths = (DATASETS['train_ds'], DATASETS['val_ds'], DATASETS['test_ds'])\n",
    "if any(not p.exists() for p in ds_paths):\n",
    "    ds = parse_base(PATH_BASE_DS, MIN_SEQ_SIZE)\n",
    "else:\n",
    "    ds = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282e4d00-c29a-48e8-a3fd-cd40db17975b",
   "metadata": {},
   "source": [
    "Then, we initialize tokenizer and uBERTaLoader. The latter won't be used directly, but we'll utilize some of its methods to prepare the MLM dataset. Namely, we'll kmerize the sequence data (sequence, its coordinates, and the experimental signal), i.e., slide a window with size three and step one over the kmerized sequence data. Finally, we'll roll the window with size `WINDOW - 2` and `STEP` defined above. We substract two from `WINDOW` to account for special tokens `CLS` and `SEP`, prepended and appended to a sequence, resp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61b2550c-cece-4368-81c5-191384a672dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DNATokenizer(kmer=KMER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c5212db-5e9c-4aed-950d-77b64f1696d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:uBERTa.loader:Using kmer 3 on ('Seq', 'SeqEnum', 'Signal', 'Classes')\n",
      "INFO:uBERTa.loader:Rolling window with size 98, step 50\n",
      "/home/ivan/code/uBERTa/uBERTa/loader.py:418: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  \" \".join(seq_chunk), np.array(cls_chunk),\n",
      "/home/ivan/code/uBERTa/uBERTa/loader.py:419: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  np.array(seq_enum_chunk), np.array(signal_chunk))\n"
     ]
    }
   ],
   "source": [
    "loader = uBERTaLoader(tokenizer=tokenizer)\n",
    "ds = loader.kmerize(ds)\n",
    "ds = loader.roll_window(ds, WINDOW - 2, STEP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efcfd5d-f394-4446-b749-ac0a5aaf9127",
   "metadata": {},
   "source": [
    "Next, we encode windowed sequences using tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4039cf66-f6a2-4e7d-88ab-ed9fa01efaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = np.array(list(map(\n",
    "    tokenizer.encode, \n",
    "    ds.Seq.apply(lambda x: x.split())\n",
    ")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4372afc8-7837-40ec-a7b6-66f4319eacb2",
   "metadata": {},
   "source": [
    "MLM objective means masking ~15% of the input tokens. We'll select ~6% of random tokens in the encoded sequences and expand the selection laterally by one position to ensure we mask (overlapping) consecutive tokens, e.g.,\n",
    "\n",
    "```\n",
    "... AAA AAA ATG ACC GGC CGG ...\n",
    "...  0   0   1   0   0   0  ...\n",
    "\n",
    "-> \n",
    "\n",
    "... AAA AAA ATG ACC GGC CGG ...\n",
    "...  0   1   1   1   0   0  ...\n",
    "```\n",
    "(We used actual kmers instead of token IDs for clarity)\n",
    "\n",
    "In some cases, this will result in masking `CLS` and `SEP` tokens which we'll \"demask\" manually. As a result, the total amount of masked tokens will be slightly less than 6% * 3 = 18%. We'll use the composed binary mask to mask the input tokens with `MASK` token, and mask the corresponding class labels with -100. Finally, we'll also create an attention mask for `PAD` tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66400927-7662-4bd5-b5b9-af183f356abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.random.binomial(1, 0.06, encoded.shape)\n",
    "mask = fill_row_around_ones(mask)\n",
    "mask[encoded == tokenizer.pad_token_id] = 0\n",
    "mask[:, 0] = 0\n",
    "mask[:, -1] = 0\n",
    "mask = mask.astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d8e2f29-20f3-4754-9556-bb5f88d51a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = encoded.copy()\n",
    "labels[~mask] = -100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef4c97bb-e476-4386-8b34-472ae44d5e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded[mask] = tokenizer.mask_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "545d35ca-5cce-48de-8c17-643c62d981d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "att_mask = (encoded != tokenizer.pad_token_id).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f203926-8beb-4342-b50f-9759785f0020",
   "metadata": {},
   "source": [
    "Below, we'll verify that:\n",
    "- Labels do not contain `MASK` tokens\n",
    "- The number of non-masked (other than -100) tokens equals to the number of masked tokens in the encoded input\n",
    "- The number of masked tokens ~15% (15.56 to be more precise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2018578-d324-46b9-9d42-eb877f266257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 5405680, 5405680, 0.15556182265017526)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(labels == 4).sum(), (labels != -100).sum(), (encoded == 4).sum(), (encoded == 4).sum() / np.prod(encoded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d79d09-4b64-47bf-b603-8900f11327ba",
   "metadata": {},
   "source": [
    "Finally, we'll wrap the encoded input, attention mask and labels into a tensor dataset. `Loader` will accept this `TensorDataset` object and split it into the training and validation subsets internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d20cab9f-651a-4277-93c3-9bb6cef48edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tds = TensorDataset(\n",
    "    torch.tensor(encoded, dtype=torch.long),\n",
    "    torch.tensor(att_mask, dtype=torch.int),\n",
    "    torch.tensor(labels, dtype=torch.long)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bef54b-7acf-41e0-9fe8-7879c8ae2dfc",
   "metadata": {},
   "source": [
    "## Setup model and loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d924e95-3ced-4053-afc6-995c872e469d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = Loader(tds, 2 ** 6)\n",
    "loader.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7dd3ef-1ddd-42a7-ba05-e98c73ad8296",
   "metadata": {},
   "source": [
    "Calculate the exact number of warmup steps based on the number of batches and the maximum training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2d051b0-150b-446c-b618-4401f18ef015",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25800, 516000)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warmup_steps, total_steps = calc_scheduler_steps(\n",
    "    loader, warmup_perc=0.05, max_epochs=MAX_EPOCHS)\n",
    "warmup_steps, total_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d43951-d17f-407a-9bf4-e3d40b8c164b",
   "metadata": {},
   "source": [
    "Initialize the config. We'll use the default configuration except reducing the model's dimension by two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5cef1578-33be-43ba-b583-e74223d66bd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertConfig {\n",
       "  \"activation\": \"gelu\",\n",
       "  \"attention_dropout\": 0.1,\n",
       "  \"dim\": 384,\n",
       "  \"dropout\": 0.1,\n",
       "  \"hidden_dim\": 3072,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"distilbert\",\n",
       "  \"n_heads\": 12,\n",
       "  \"n_layers\": 6,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"qa_dropout\": 0.1,\n",
       "  \"seq_classif_dropout\": 0.2,\n",
       "  \"sinusoidal_pos_embds\": false,\n",
       "  \"transformers_version\": \"4.16.2\",\n",
       "  \"vocab_size\": 69\n",
       "}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = DistilBertConfig()\n",
    "config.vocab_size = tokenizer.vocab_size\n",
    "config.dim = config.dim // 2\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaf84a6-6bbc-4634-8ae0-7d618d5a8163",
   "metadata": {},
   "source": [
    "`uBERTa_mlm` is a lightning module that encapsulates the model, its config, setups for optimizer and scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d974e878-7ba6-4b2a-81de-246b756f5b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = uBERTa_mlm(\n",
    "    model=DistilBertForMaskedLM,\n",
    "    config=config,\n",
    "    opt_kwargs={'lr': 1e-5, 'weight_decay': 0.01, 'eps': 1e-8}, \n",
    "    scheduler=get_linear_schedule_with_warmup,\n",
    "    scheduler_kwargs={'num_warmup_steps': warmup_steps, 'num_training_steps': total_steps},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "14496ced-afcd-49e1-a46a-9b9136bea7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24786/3704423482.py:1: LightningDeprecationWarning: The `LightningModule.summarize` method is deprecated in v1.5 and will be removed in v1.7. Use `pytorch_lightning.utilities.model_summary.summarize` instead.\n",
      "  model.summarize()\n",
      "/home/ivan/miniconda3/envs/uberta/lib/python3.8/site-packages/pytorch_lightning/utilities/model_summary.py:471: LightningDeprecationWarning: Argument `mode` in `LightningModule.summarize` is deprecated in v1.4 and will be removed in v1.6. Use `max_depth=1` to replicate `mode=top` behavior.\n",
      "  rank_zero_deprecation(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  | Name  | Type                  | Params\n",
       "------------------------------------------------\n",
       "0 | model | DistilBertForMaskedLM | 18.1 M\n",
       "------------------------------------------------\n",
       "18.1 M    Trainable params\n",
       "0         Non-trainable params\n",
       "18.1 M    Total params\n",
       "72.426    Total estimated model params size (MB)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9df870a-57f5-479c-9a45-6c3ca3d6a2da",
   "metadata": {},
   "source": [
    "## Train and save the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c8fecc7-93b4-4881-a28a-2577501f2c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopper = pl.callbacks.early_stopping.EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    verbose=True, mode='min', \n",
    "    min_delta=1e-6,\n",
    "    patience=20)\n",
    "pointer = pl.callbacks.ModelCheckpoint(\n",
    "    monitor='val_loss', \n",
    "    dirpath=f'../models/checkpoints/{MODEL}', \n",
    "    verbose=True, mode='min')\n",
    "logger = pl.loggers.TensorBoardLogger('../logs', f'{MODEL}')\n",
    "lr_monitor = pl.callbacks.LearningRateMonitor('epoch')\n",
    "bar = pl.callbacks.TQDMProgressBar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b4fa478e-ede3-40a0-8431-4ff3c0632856",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = [0]\n",
    "trainer = pl.Trainer(\n",
    "    gradient_clip_val=1.0, \n",
    "    # stochastic_weight_avg=True,\n",
    "    accelerator=\"gpu\",\n",
    "    precision=16,\n",
    "    gpus=gpus,\n",
    "    callbacks=[stopper, pointer, bar, lr_monitor],\n",
    "    logger=logger,\n",
    "    max_epochs=MAX_EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f29384-de38-4c59-ad73-b17d24e98b26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.fit(model, loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ecaf54-5e35-4eb1-adf9-71815e4a10e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.save_pretrained(MODEL_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
