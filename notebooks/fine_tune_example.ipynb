{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fad9aad3-98e4-45d7-9ca8-7b83da820993",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0b25a49-2c30-45f9-a60e-e13219443946",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import typing as t\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from toolz import curry\n",
    "from transformers import BertConfig, DNATokenizer\n",
    "from uBERTa.base import ModelSetup, RunSetup, OptSetup, StopSetup\n",
    "from uBERTa.datasets_generator import DatasetGenerator, prepare_sequences, train_test_split\n",
    "from uBERTa.model import uBERTa, BertCentralPooler\n",
    "from uBERTa.utils import train, setup_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac97a205-8d80-4f26-aeb6-14770d6fb118",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79398157-d4d0-4617-9b97-20610e71c8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to a reference, log file and the trained model\n",
    "REF, LOG, MODEL, = (\n",
    "    Path('hg38.fa'), Path('log.txt'), Path('models/trained/3-new-12w-0/'))\n",
    "# Paths to a full, eval and train datasets\n",
    "DS, DEV, TRAIN = (\n",
    "    Path('DS_BASE.tsv'), Path('DEV_BASE.tsv'), Path('TRAIN_BASE.tsv'))\n",
    "# Kmer size, flank size, and the fraction of the eval examples\n",
    "KMER, FLANK, DEV_FRAC = 3, 100, 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00b2d1a7-0484-4296-a604-0c0a704b0c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGGER = setup_logger(\n",
    "    './log2.txt', file_level=logging.DEBUG, \n",
    "    stdout_level=logging.INFO, \n",
    "    stderr_level=logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693e4f03-3d23-4cca-8361-62c5e409d301",
   "metadata": {},
   "source": [
    "# Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62a50c56-45a8-4e9e-b950-107215d97a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the generator\n",
    "dsg = DatasetGenerator(\n",
    "    DS, REF, \n",
    "    neg_multiplier=4, \n",
    "    neg_fractions=(\n",
    "        0.00,  # Completely random samples \n",
    "        0.05,  # Random samples centered on start codons\n",
    "        0.70,  # Valid uORFs without experimental support\n",
    "        0.25,  # Valid uORFs with experimental support\n",
    "    ),\n",
    "    pos_fractions=(\n",
    "        1.0,  # u samples  (uORF start codons)\n",
    "        1.0,  # ma samples (alternative start codons)\n",
    "        1.0   # m samples  (CDS start codons)\n",
    "    ),\n",
    "    flank_size=1,  # This will fetch just the start codon\n",
    "    kmer_size=KMER\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6287c65-7078-4a37-8e8c-01373ad211c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets(dev_path, train_path, dsg, flank_size, kmer_size, dev_fraction=0.2):\n",
    "    if dev_path.exists() and train_path.exists():\n",
    "        dev, train = map(\n",
    "            lambda p: pd.read_csv(p, sep='\\t'), \n",
    "            [dev_path, train_path])\n",
    "    else:\n",
    "        dev, train = train_test_split(dsg(), dev_fraction)\n",
    "    _prepare = curry(prepare_sequences)(\n",
    "        ref=dsg.ref, flank_size=flank_size, kmer_size=kmer_size)\n",
    "    dev, train = map(_prepare, [dev, train])\n",
    "    return dev, train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0db9e60a-d319-497f-b2b3-b088653d1e4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    5192\n",
      "1    1278\n",
      "Name: IsPositive, dtype: int64\n",
      "0    23399\n",
      "1     5872\n",
      "Name: IsPositive, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Either read or generate datasets\n",
    "dev_ds, train_ds = prepare_datasets(DEV, TRAIN, dsg, FLANK, KMER, DEV_FRAC)\n",
    "print(dev_ds.IsPositive.value_counts(), \n",
    "      train_ds.IsPositive.value_counts(), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8be7588-beac-40e4-bfbb-084cc2edd9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, save the datasets \n",
    "dev_ds.to_csv(DEV, sep='\\t', index=False)\n",
    "train_ds.to_csv(TRAIN, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa059871-a7fb-4bce-a2bc-6fd156caba40",
   "metadata": {},
   "source": [
    "# Setup model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37bb4096-324f-41f0-9b05-5c1130d631e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'ATG': 14510, 'CTG': 7316, 'GTG': 2878, 'ACG': 1355, 'TTG': 1348, 'ATC': 717, 'ATT': 684, 'ATA': 193, 'AGG': 184, 'AAG': 86})\n",
      "Counter({'ATG': 3254, 'CTG': 1563, 'GTG': 636, 'ACG': 314, 'TTG': 296, 'ATC': 167, 'ATT': 141, 'ATA': 51, 'AGG': 30, 'AAG': 18})\n"
     ]
    }
   ],
   "source": [
    "# Derive and validate the central token position\n",
    "central_codon = len(dev_ds.Seq.iloc[0].split(' ')) // 2\n",
    "print(Counter(s.split(' ')[central_codon] for s in train_ds.Seq), \n",
    "      Counter(s.split(' ')[central_codon] for s in dev_ds.Seq), \n",
    "      sep='\\n')\n",
    "central_token = central_codon + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "959d1beb-9302-4432-9786-000d6738a3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_setup = ModelSetup(BertConfig, uBERTa, DNATokenizer, str(MODEL))\n",
    "# Batch size, Num epochs, Fraction of warmup steps\n",
    "# run_setup = RunSetup(16, 30, 0.1)\n",
    "run_setup = RunSetup(16, 2, 0.1)\n",
    "# Learning rate, Epsilon, Betas, Weight decay\n",
    "opt_setup = OptSetup(5e-6, 1e-8, (0.9, 0.999), 0.1)\n",
    "# Number of rounds, Improvement\n",
    "stop_setup = StopSetup(10, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73acc4e2-01aa-4f2a-8827-4f59cbcc0cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_kwargs = dict(\n",
    "    finetuning_task='uORF',\n",
    "    hidden_dropout_prob=0.2,\n",
    ")\n",
    "model_kwargs = dict()\n",
    "tokenizer_kwargs = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fe657c3-04c6-4b93-b616-c79f6f6ec2c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-11 16:31:40,830 INFO [configuration_utils--get_config_dict]: loading configuration file models/trained/3-new-12w-0/config.json\n",
      "2021-12-11 16:31:40,831 INFO [configuration_utils--from_dict]: Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"finetuning_task\": \"uORF\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.2,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"num_rnn_layer\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"rnn\": \"lstm\",\n",
      "  \"rnn_dropout\": 0.0,\n",
      "  \"rnn_hidden\": 768,\n",
      "  \"split\": 10,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 69\n",
      "}\n",
      "\n",
      "============================================================\n",
      "<class 'transformers.tokenization_dna.DNATokenizer'>\n",
      "2021-12-11 16:31:40,833 INFO [tokenization_utils--_from_pretrained]: Model name 'models/trained/3-new-12w-0' not found in model shortcut name list (dna3, dna4, dna5, dna6). Assuming 'models/trained/3-new-12w-0' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "2021-12-11 16:31:40,847 INFO [tokenization_utils--_from_pretrained]: Didn't find file models/trained/3-new-12w-0/added_tokens.json. We won't load it.\n",
      "2021-12-11 16:31:40,865 INFO [tokenization_utils--_from_pretrained]: loading file models/trained/3-new-12w-0/vocab.txt\n",
      "2021-12-11 16:31:40,866 INFO [tokenization_utils--_from_pretrained]: loading file None\n",
      "2021-12-11 16:31:40,867 INFO [tokenization_utils--_from_pretrained]: loading file models/trained/3-new-12w-0/special_tokens_map.json\n",
      "2021-12-11 16:31:40,868 INFO [tokenization_utils--_from_pretrained]: loading file models/trained/3-new-12w-0/tokenizer_config.json\n",
      "2021-12-11 16:31:40,871 INFO [modeling_utils--from_pretrained]: loading weights file models/trained/3-new-12w-0/pytorch_model.bin\n",
      "2021-12-11 16:31:42,448 INFO [modeling_utils--from_pretrained]: Weights of uBERTa not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "2021-12-11 16:31:42,449 INFO [modeling_utils--from_pretrained]: Weights from pretrained model not used in uBERTa: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n"
     ]
    }
   ],
   "source": [
    "config = model_setup.Config.from_pretrained(\n",
    "    model_setup.ModelPath, **config_kwargs)\n",
    "config.central_position = central_token\n",
    "\n",
    "tokenizer = model_setup.Tokenizer.from_pretrained(\n",
    "    model_setup.ModelPath, **tokenizer_kwargs)\n",
    "model = model_setup.Model.from_pretrained(\n",
    "    model_setup.ModelPath, config=config, **model_kwargs)\n",
    "model.bert.pooler = BertCentralPooler(config)\n",
    "model.to('cuda');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d7e84f-3a7c-4f30-9033-db6cd41053f8",
   "metadata": {},
   "source": [
    "# Fine-tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ed8fc58-5f7f-418b-9699-6a035cff4ab4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7c01c6280d94ee0afc515b5174e1a28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running epochs:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2611b4002e8d4eb88f46d53c3c03fca7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running train batches:   0%|          | 0/183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ivan/miniconda3/envs/orf2/lib/python3.6/site-packages/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272115665/work/torch/csrc/utils/python_arg_parser.cpp:1050.)\n",
      "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-11 16:32:44,756 INFO [utils--train]: Finished epoch 1 with loss 115.26578903198242 and (unweighted) scores Scores(acc=0.5490263067987701, roc_auc=0.4665075059112488, f1=0.23787528868360278, prec=0.1840929401251117, rec=0.3360522022838499)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99774fda48d94f63a1cabef6f7e893e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running predict batches:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-11 16:32:48,784 INFO [utils--train]: Fininshed evaluation with loss 23.58084124326706 and scores Scores(acc=0.7002032520325203, roc_auc=0.7827996028051883, f1=0.6844919786096262, prec=0.7223476297968409, rec=0.6504065040650407)\n",
      "2021-12-11 16:32:48,787 INFO [utils--__call__]: The model has improved the score by 0.7827996028051883-0=0.7827996028051883\n",
      "2021-12-11 16:32:48,789 INFO [configuration_utils--save_pretrained]: Configuration saved in checkpoint_cache/config.json\n",
      "2021-12-11 16:32:51,006 INFO [modeling_utils--save_pretrained]: Model weights saved in checkpoint_cache/pytorch_model.bin\n",
      "2021-12-11 16:32:55,353 INFO [utils--train]: Dumped the current state to checkpoint_cache\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6067d0119c054662afaa3a33b2fc7e1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running train batches:   0%|          | 0/183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-11 16:33:50,522 INFO [utils--train]: Finished epoch 2 with loss 98.80516465008259 and (unweighted) scores Scores(acc=0.5937820293816194, roc_auc=0.5147100914921726, f1=0.2709993868792152, prec=0.21709233791748528, rec=0.3605220228384992)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f25be9873926436c9b43fdd6903da13b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running predict batches:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-11 16:33:54,558 INFO [utils--train]: Fininshed evaluation with loss 23.305896788835526 and scores Scores(acc=0.7080850865760565, roc_auc=0.7881524235089679, f1=0.6928671123191074, prec=0.7309739085507634, rec=0.6585365853658538)\n",
      "2021-12-11 16:33:54,560 INFO [utils--__call__]: The model hasn't improved the score 0.7827996028051883 for 1 rounds out of 10\n"
     ]
    }
   ],
   "source": [
    "# results = train(\n",
    "#     model, tokenizer, train_ds, dev_ds, \n",
    "#     run_setup, opt_setup, stop_setup, \n",
    "#     Path('checkpoint_cache'))\n",
    "scores_train, scores_eval = train(\n",
    "    model, tokenizer, \n",
    "    train_ds.sample(int(0.1 * len(train_ds))), \n",
    "    dev_ds.sample(int(0.1 * len(dev_ds))), \n",
    "    run_setup, opt_setup, stop_setup, \n",
    "    Path('checkpoint_cache'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "187b8414-b613-4064-8cd9-ed7a3ca67d5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.549026</td>\n",
       "      <td>0.466508</td>\n",
       "      <td>0.237875</td>\n",
       "      <td>0.184093</td>\n",
       "      <td>0.336052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.593782</td>\n",
       "      <td>0.514710</td>\n",
       "      <td>0.270999</td>\n",
       "      <td>0.217092</td>\n",
       "      <td>0.360522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        acc   roc_auc        f1      prec       rec\n",
       "0  0.549026  0.466508  0.237875  0.184093  0.336052\n",
       "1  0.593782  0.514710  0.270999  0.217092  0.360522"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(scores_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c2c7237-a666-4b13-a8fe-7e34eae65508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.700203</td>\n",
       "      <td>0.782800</td>\n",
       "      <td>0.684492</td>\n",
       "      <td>0.722348</td>\n",
       "      <td>0.650407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.708085</td>\n",
       "      <td>0.788152</td>\n",
       "      <td>0.692867</td>\n",
       "      <td>0.730974</td>\n",
       "      <td>0.658537</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        acc   roc_auc        f1      prec       rec\n",
       "0  0.700203  0.782800  0.684492  0.722348  0.650407\n",
       "1  0.708085  0.788152  0.692867  0.730974  0.658537"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(scores_eval)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
